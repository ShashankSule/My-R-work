---
title: "Solutions to Homework 3"
author: "Shashank Sule"
date: "9/10/2019"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amsthm}
   - \usepackage{amssymb}
   - \usepackage{amsfonts}
   - \usepackage[margin=1in]{geometry}
output: pdf_document
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(LearnBayes)
require(ggplot2)
require(reshape2)
```

# Problem 1 

a) In a Beta distribution parametrized as $\text{Beta}(a,b)$, $a$ represents the number of successes and $b$ the number of failures. So if there were $8$ successes and $2$ failures out of a data of $10$ then Alex has a prior $\text{Beta}(a,b)$.

b) We compute the parameters using the \texttt{beta.select} function: 

```{r, echo=FALSE}
quantile1=list(p=.2,x=0.3)
quantile2=list(p=.9,x=0.4)
parameters <- beta.select(quantile1,quantile2)
alpha <- parameters[1]
beta <- parameters[2]
parameters
```

c) Note that $y=692$, $n=1048$ and the posterior distribution is $\text{Beta}(a + y, b + n - y)$. So Alex has posterior $\text{Beta}(700, 358)$ and Benedict has posterior $\text{Beta}(726.95, 423.98)$. 

The prior: 
```{r, echo=FALSE}
p <- seq(from=0, to=1, by=0.01)
Alex <- sapply(p, dbeta, shape1 = 8, shape2 = 2)
Benedict <- sapply(p, dbeta, shape1 = alpha, shape2 = beta)
prior <- data.frame(p, Alex, Benedict)
priort <- melt(prior, id="p")
ggplot(data=priort, aes(x=p, y=value, color=variable))+
  geom_line()+
  labs(title="Comparing priors", x="p", y="Density", color="Prior")
```


```{r, echo=FALSE}
p <- seq(from=0, to=1, by=0.001)
AlexPos <- sapply(p, dbeta, shape1 = 8+692, shape2 = 2+1048-692)
BenedictPos <- sapply(p, dbeta, shape1 = alpha+692, shape2 = beta + 1048-692)
posterior <- data.frame(p, AlexPos, BenedictPos)
posteriort <- melt(posterior, id="p")
ggplot(data=posteriort, aes(x=p, y=value, color=variable))+
  geom_line()+
  labs(title="Comparing Posteriors", x="p", y="Density")+
  scale_color_discrete(name="Posterior", labels=c("Alex", "Benedict"))
```

Credible intervals for Alex: 

```{r, echo=FALSE}
qbeta(c(0.025,0.975), shape1 = 8+692, shape2 = 2 + 1048 - 692)
```

Credible intervals for Benedict:

```{r, echo=FALSE}
qbeta(c(0.025,0.975), shape1 = alpha+692, shape2 = beta + 1048 - 692)
```

d) To conduct a prior predictive check, we sample values from each prior and then sample a value from the likelihood function. Then we look for clustering of the sampled data around the actual data. I'll do a simulation for $1000$ trials.

```{r, echo=FALSE}
set.seed(123)
S <- 1000 
a <- rbeta(S, 8, 2)
b <- rbeta(S, alpha, beta)
alex <- sapply(a, rbinom, n=1, size = 1048)
benedict <- sapply(b, rbinom, n=1, size=1048)
```
Alex's probability values: 

```{r}
c(sum(alex >= 692)/S, 1 - sum(alex >= 692)/S)
```

Benedict's probability values: 
```{r}
c(sum(benedict >= 692)/S, 1 - sum(benedict >= 692)/S)
```

Thus, neither have a good prior distribution for prediction but Benedict's is worse than Alex's because Benedict's probabiities are far more extreme (all of Benedict's data lies on one side of the actual observed value). 

# Problem 2 

To get the odds data, I'll use Monte Carlo sampling and then transform the data. The summarized data is as follows:

```{r,echo=FALSE}
apos <- rbeta(S, 8+692, 2+1048-692)
bpos <- rbeta(S, alpha+692, beta+1048-692)
odda <- apos/(1-apos)
oddb <- bpos/(1-bpos)
meana <- mean(odda)
meanb <- mean(oddb)
mediana <- median(odda)
medianb <- median(oddb)
qa <- quantile(odda, probs = c(0.025,0.975))
qb <- quantile(oddb, probs = c(0.025,0.975))
dats <- matrix(data = NA, nrow = 2, ncol = 4, byrow = TRUE, dimnames = list(c("Alex","Benedict"),c("Mean","Median", "2.5%", "97.5%")))
dats[1,] <- c(meana, mediana, qa[1],qa[2])
dats[2,] <- c(meanb, medianb, qb[1],qb[2])
dats
```

On average, Alex's odds of kids having a TV in their room are slightly igher than those of Benedict. 

# Problem 3 
```{r, echo=FALSE}
S <- c(10,100,500,1000,5000)
left <- numeric(length = length(S))
right <- numeric(length = length(S))
for(i in 1:length(S)){
  left[i] <- quantile(rbeta(S[i],15.06, 10.56), probs = c(0.05, 0.95))[1]
  right[i] <- quantile(rbeta(S[i],15.06, 10.56), probs = c(0.05, 0.95))[2]
}
data <- data.frame(S,left,right)
names(data) <- c("S","5%", "95%")
data
```

I computed $S = \{10, 100, 500, 1000, 5000\}$ random samples from $\text{Beta}(15.06,10.56)$ using the \texttt{rbeta} function and then computed the quantiles of the data using \texttt{quantile}. As the number of samples gets larger, the interval gets closer to the actual credible interval. 

# Problem 4 

a) Note that given the means $\mu_A$ and $\mu_B$, the data follows a normal likelihood. Now if we sample independently from both the accelerated and non-accelerated groups then the joint likelihood of the data is given by the product of their individual likelihood functions. First of all, from the formulas derived in class we set $\phi = 1/16$ and $n=6$ to get that

$$ L_A(\mu_A) = \Big(\frac{1}{32\pi}\Big)^{3}e^{-\frac{1}{32}\sum_{i=1}^{6}(y_{A,i} - \mu_A)^2}$$
and 


$$ L_B(\mu_B) = \Big(\frac{1}{32\pi}\Big)^{3}e^{-\frac{1}{32}\sum_{j=1}^{6}(y_{B,j} - \mu_B)^2}$$

The joint likelihood is obtained as a product of the individual likelihoods, so 

$$ L_{A,B}(\mu_A, \mu_B) = L_A(\mu_A)L_B(\mu_B) = \Big(\frac{1}{32\pi}\Big)^{6}e^{-\frac{1}{32}\sum_{j=1}^{6}(y_{A,j} - \mu_A)^2 -\frac{1}{32}\sum_{j=1}^{6}(y_{B,j} - \mu_B)^2}$$  

b) If the joint prior for $(\mu_A, \mu_B)$ is independent then the density function is a product of the individual priors, i.e $\pi(\mu_A, \mu_B) = \pi(\mu_A)\pi(\mu_B)$. Thus, the joint posterior can be obtained as follows: 

$$ \pi(\mu_A, \mu_B \mid \textbf{Y}_A, \textbf{Y}_B, \sigma) = \frac{L_{A,B}(\mu_A,\mu_b)\pi_{A,B}(\mu_A,\mu_B)}{\int_{\mathbb{R^2}}L_{A,B}(x,y)\pi_{A,B}(x,y)\,dA}$$
Since both the joint likelihood and prior are products of the marginal likelihood and prior, the integral in the denominator is separable. 

$$ = \frac{L_A(\mu_A)\pi_A(\mu_A)L_B(\mu_b)\pi_B(\mu_B)}{\int_{\mathbb{R}}L_A(x)\pi_A(x)\,dx\int_{R}L_B(y)\pi_B(y)\,dy} = \Big(\frac{L_A(\mu_A)\pi_A(\mu_A)}{\int_{\mathbb{R}}L_A(x)\pi_A(x)\,dx}\Big)\Big(\frac{L_B(\mu_B)\pi_B(\mu_B)}{\int_{\mathbb{R}}L_B(y)\pi_B(y)\,dy}\Big) $$ 

$$ = \pi_A(\mu_A \mid \textbf{Y}_A, \sigma)\pi_B(\mu_B \mid \textbf{Y}_B, \sigma)$$

Finally, from the result in class, the family of normal distributions forms a conjugate prior under a normal likelihood where the posterior distribution has mean $\frac{\phi_0\mu_0 + n\phi\bar{y}}{\phi_0 + n\phi}$ and standard deviation $\sqrt{\frac{1}{\phi_0 + n\phi}}$. Thus, in this case we have 

$$ \mu_A \mid \textbf{Y}_A, \sigma \sim N\bigg(\frac{\phi_A\gamma_A + (6/16)15.2}{\phi_A + (6/16)}, \sqrt{\frac{1}{\phi_A + (6/16)}}\,\bigg)$$

and 

$$ \mu_B \mid \textbf{Y}_B, \sigma \sim N\bigg(\frac{\phi_B\gamma_B + (6/16)6.2}{\phi_B + (6/16)}, \sqrt{\frac{1}{\phi_B + (6/16)}}\,\bigg) $$

# Problem 5 

a) As stated in the hint, I compute $\delta = \mu_A - \mu_B$ and check the proportion using $S=100000$ monte carlo draws. 

```{r}
S <- 10000
n <- 6 
phi <- 1/16
phi_A <- (1/(20)^2)
mu_A0 <- 0
ybar_A <- 15.2
phi_B <- (1/(20)^2)
mu_B0 <- 0
ybar_B <- 6.2 
delta <- rnorm(S, mean = ((phi_A*mu_A0 + n*phi*ybar_A)/(phi_A + n*phi)), sd = sqrt(1/(phi_A + n*phi))) - rnorm(S, mean = ((phi_B*mu_B0 + n*phi*ybar_B)/(phi_B + n*phi)), sd = sqrt(1/(phi_B + n*phi))) 
```

```{r}
ggplot(as.data.frame(delta), aes(x=delta)) + 
  geom_histogram()
```
```{r}
sum(delta > 0)/S
```

Thus, the average improvement for the accelerated group is larger than that for the no growth group since most of $\delta$ lies above 0 (using monte carlo simulation). 

b) As suggested in the hint, I'll draw a posterior predictive distribution using the posteriors of $\mu_A$ and $\mu_B$ and their respective likelihoods. Then I'll find the proportion of positive draws from the $\widetilde{Y}_A - \widetilde{Y}_B$. 

```{r}
A_means <- numeric(length = S)
B_means <- numeric(length = S)

for(i in 1:S){
  mu_A <- rnorm(1, mean = ((phi_A*mu_A0 + n*phi*ybar_A)/(phi_A + n*phi)), sd = sqrt(1/(phi_A + n*phi)))
  mu_B <- rnorm(1, mean = ((phi_B*mu_B0 + n*phi*ybar_B)/(phi_B + n*phi)), sd = sqrt(1/(phi_B + n*phi)))
  A_means[i] <- sum(rnorm(6, mean = mu_A, sd = 4))/6
  B_means[i] <- sum(rnorm(6, mean = mu_B, sd = 4))/6
}
```

```{r}
differences <- A_means - B_means 
mean(differences > 0)
```

Thus, most values of $\widetilde{Y}_A$ are higher than those in $\widetilde{Y}_B$ a child randomly assigned to the accelerated treatment will show better IQ improvement than a child randomly assigned to the normal treatment. 
