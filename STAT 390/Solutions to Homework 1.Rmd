---
title: "Solutions to Homework 1"
author: "Shashank Sule"
date: "9/10/2019"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amsthm}
   - \usepackage{amssymb}
   - \usepackage{amsfonts}
output: pdf_document
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1 

a. Let $S$ be the event that an email is spam and $M$ be the event that an email is marked as spam. Then 
$$ P(M) = P(M|S)P(S) + P(M|S^{c})P(S^{c}) = (0.92)(0.08) + (0.03)(0.92) = 0.1012$$
```{r}
(0.92)*(0.08) + (0.03)*(0.92)
```

b. We can calculate $P(S|M)$ via Bayes' theorem:

$$ P(S|M) = \frac{P(M|S)P(S)}{P(M|S)P(S) + P(M|S^{c})P(S^{c})} = 0.73 $$
```{r}
(0.92 * 0.08)/((0.92)*(0.08) + (0.03)*(0.92))
```

# Problem 2 

```{r}
# Making the data matrix 

vals <- c(0.018,0.035,0.031,0.008,0.018,0.002,0.112,0.064,0.032,0.069,0.001,0.066,0.094,0.032,0.084,0.001,0.018,0.019,0.010,0.051,0.001,0.029,0.032,0.043,0.130)

joint <- matrix(vals, nrow = 5, byrow=TRUE, dimnames = list(c("farm", "operatives","craftsmen","sales","professional"), c("farm", "operatives","craftsmen","sales","professional")))
joint
```

a) Let $F$ be the father's occupation and $S$ the son's. Then the marginal probability distribution of the father's occupation is given by $P(F = f_i) = \sum_{s_i \in S}P(F = f_i, S = s_i)$. The marginal probability distribution of the father's occupation:

```{r}
mfather <- numeric(length = 5)
rvarb <- c("farm", "operatives","craftsmen","sales","professional")
for(i in 1:5)
{
  mfather[i] = sum(joint[i,])
}
names(mfather) <- rvarb 
mfather
```

b) The marginal probability distribution of the son's occupation: 

```{r}
mson <- numeric(length = 5)
rvarb <- c("farm", "operatives","craftsmen","sales","professional")
for(i in 1:5)
{
  mson[i] = sum(joint[,i])
}
names(mson) <- rvarb 
mson
```

c) The conditional distribution $P(S=s_i\,|\,F = f_i)$ is given as 

$$ P(S=s_i\,|\,F = f_i) = \frac{P(S=s_i, F=f_i)}{P(F = f_i)}$$

Hence the conditional distribution of the son's occupation given that the father is a farmer is as follows: 

```{r}
cson <- joint["farm",]/mfather["farm"]
cson
```

d) Similarly, the father's conditional distribution given that the son is a farmer is as follows:

```{r}
cfather <- joint[,"farm"]/mson["farm"]
cfather
```

# Problem 3 

Set $Y ~ N(\mu, \sigma)$ and $Z = \frac{X-\mu}{\sigma}$. Then we consider the cumulative distribution function $F_{Z}(z)$, which is given as follows: 

$$ F_{Z}(z) = P(Z \leq z) = P(\frac{X-\mu}{\sigma} \leq z) = P(X \leq \sigma z + \mu) = \int_{-\infty}^{\sigma z + \mu} \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\big(\frac{x-\mu}{\sigma}\big)^2}\,dx$$
Then we know that the density function $f_{Z}(z) = F'_{z}(z)$ so we have that 

$$ f_{Z}(Z) = \frac{d}{dz}\int_{-\infty}^{\sigma z + \mu} \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\big(\frac{x-\mu}{\sigma}\big)^2}\,dx = \frac{\sigma}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\big(\frac{\sigma z + \mu -\mu}{\sigma}\big)^2}\,dx = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}z^2}\,dx$$
Thus $Z$ has a Gaussian density function with $\mu = 0$ and $\sigma = 1$ and so $Z ~ N(0,1)$ 

# Problem 4

From the conditional independence formula we have that 

$$ P(X_1 = x_1, \ldots, Y_m = y_m | \mu_X, \sigma_X, \mu_Y, \sigma_Y)$$
$$= \prod^{n}_{i=1}P(X_i = x_i|\mu_X, \sigma_X, \mu_Y, \sigma_Y)\prod^{m}_{i=1}P(Y_i = y_i|\mu_X, \sigma_X, \mu_Y, \sigma_Y)$$
$$ = \prod^{n}_{i=1}P(X_i = x_i|\mu_X, \sigma_X)\prod^{m}_{i=1}P(Y_i = y_i|\mu_Y, \sigma_Y)$$
$$= \frac{1}{\sigma_{X}^n(2\pi)^{n/2}}e^{-\frac{1}{2}\frac{\sum_{i=1}^n(x_i - \mu_X)^2}{\sigma^2_X}}\frac{1}{\sigma_{Y}^n(2\pi)^{m/2}}e^{-\frac{1}{2}\frac{\sum_{i=1}^m(y_i - \mu_Y)^2}{\sigma^2_Y}}$$

$$ = \frac{1}{\sigma_{X}^n\sigma_{Y}^m(2\pi)^{(n+m)/2}}e^{-\frac{1}{2}
\Bigg(\frac{\sum_{i=1}^n(x_i - \mu_X)^2}{\sigma^2_X} + \frac{\sum_{i=1}^m(y_i - \mu_Y)^2}{\sigma^2_Y}\Bigg)}$$

# Problem 5

Let $F(x) = \frac{e^x}{1 + e^x} = \frac{1}{1 + e^{-x}}$. Then $f(x) = F'(x) = \frac{e^{-x}}{(1 + e^{-x})^2}$

```{r, echo=FALSE}
linrange <- seq(-2,2, length.out = 100)
logis <- function(x) { return(exp(-x)/(1 + exp(-x))^2) }
normdis <- function(x) { (1/((2*pi)^(1/2)))*exp(-0.5*x^2)}
plot(linrange, logis(linrange), type = "l", ylim = c(0,0.5), col="red", ann= FALSE)
lines(linrange, normdis(linrange), col = "blue")
title(xlab="x")
title(ylab="y")
legend("topright",c("Logistic", "Normal"), col = c("red","blue"), pch = c("l","l"))
title(main="Comparing pdfs of logistic and normal distributions")

```

The logistic distribution is more likely to sample extreme values as the probabilities at the extremes are higher than those given by the normal distribution. 

# Problem 6


Here we calculate the posterior distribution given $n=20$ and $y=8$:

```{r, results = 'asis'}
require(knitr)
#Entering data and the prior probabilities 
priorvalues <- c(0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1)
priorprob <- c(1/23, 1/23, 7/23, 7/23, 3/23, 3/23, 1/23, 0/23, 0/23, 0/23, 0/23)

n <- 20
y <- 8

#vector for storing results
jointprob <- numeric(length = length(priorvalues))

for(i in 1:length(priorvalues))
{
  
  #compute Binomial probability given value of p - likelihood
  binomprob <- dbinom(y, n, p = priorvalues[i])
  
  #compute joint probability - posterior
  jointprob[i] <- binomprob * priorprob[i]
  
}

#compute marginal probability of y 
pofy <- sum(jointprob)

#compute posterior probabilities
posteriorprob <- jointprob/pofy

#visualizing the posterior

#put posterior probabilities in one matrix object for easy viewing 
allnighterposterior <- as.data.frame(cbind(priorvalues, priorprob, posteriorprob))
names(allnighterposterior) <- c("p", "prior", "posterior")

#list the final posterior distribution, based on our prior derived in class
#allnighterposterior

#plot the prior and posterior probabilities
require(ggplot2)
require(reshape2)

allnighterposterior_all <- melt(allnighterposterior, id = "p")

ggplot(allnighterposterior_all, aes(x = p, y = value, colour = variable)) +
  geom_point(size = 3) + 
  xlab("p") + ylab("probability") +
 theme_bw(base_size = 12, base_family = "")
kable(allnighterposterior, caption="Posterior using one-step updating")
```

In the following code, we assign the posterior calculated in class as prior and compute the posterior for $n=10$ and $y=5$.


```{r, results = 'asis'}
#Calculating posterior (n=10, y=3) from the prior distribution given in class
require(knitr)
priorvalues <- c(0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1)
priorprob <- c(1/23, 1/23, 7/23, 7/23, 3/23, 3/23, 1/23, 0/23, 0/23, 0/23, 0/23)

n <- 10
y <- 3

#vector for storing results
jointprob <- numeric(length = length(priorvalues))

for(i in 1:length(priorvalues))
{
  
  #compute Binomial probability given value of p - likelihood
  binomprob <- dbinom(y, n, p = priorvalues[i])
  
  #compute joint probability - posterior
  jointprob[i] <- binomprob * priorprob[i]
  
}

#compute marginal probability of y 
pofy <- sum(jointprob)

#compute posterior probabilities
posteriorprob <- jointprob/pofy


#Now we will do a sequential update by settting the prior probabilities to the computed posterior probabilities and then setting n=10 and y=5

priorprob <- posteriorprob
n <- 10
y <- 5


jointprob <- numeric(length = length(priorvalues))

for(i in 1:length(priorvalues))
{
  
  #compute Binomial probability given value of p - likelihood
  binomprob <- dbinom(y, n, p = priorvalues[i])
  
  #compute joint probability - posterior
  jointprob[i] <- binomprob * priorprob[i]
  
}

#compute marginal probability of y 
pofy <- sum(jointprob)

#compute posterior probabilities
posteriorprob <- jointprob/pofy
allnighterposterior <- as.data.frame(cbind(priorvalues, priorprob, posteriorprob))
names(allnighterposterior) <- c("p", "prior", "posterior")

#list the final posterior distribution, based on our prior derived in class
#allnighterposterior

#plot the prior and posterior probabilities
require(ggplot2)
require(reshape2)

allnighterposterior_all <- melt(allnighterposterior, id = "p")

ggplot(allnighterposterior_all, aes(x = p, y = value, colour = variable)) +
  geom_point(size = 3) + 
  xlab("p") + ylab("probability") +
 theme_bw(base_size = 12, base_family = "")

kable(allnighterposterior, caption="Posterior using sequential updating")
```

Thus we see that the posterior with the bigger data set calculated in one update is the same as the posterior calculated by sequential updates of 10 at a time. This is because we assume that the trials are independent so the fact that out of 10 people 3 stayed up last year does not affect the fact that out of 10 other people, 5 stayed up last year. 