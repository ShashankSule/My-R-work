---
title: "Bayesian linear regression"
author: "Jingchen (Monika) Hu"
date: "MATH 347 Bayesian Statistics"
output:
  pdf_document: default
  html_document:
    number_sections: yes
institute: Vassar College
fontsize: 11pt
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(gridExtra)
require(ProbBayes)
require(tidyverse)
require(runjags)
require(coda)
crcblue <- "#2905a1"
knitr::opts_chunk$set(echo = TRUE)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

# Installing the necessary packages

```{r, eval = FALSE}
install.packages("devtools")
require(devtools)
devtools::install_github("bayesball/ProbBayes")
require(utils)
require(ggplot2)
require(gridExtra)
require(ProbBayes)
require(tidyverse)
crcblue <- "#2905a1"
```



# Introduction: Adding a continuous predictor variable

## The simple linear regression model

```{r fig.align = "center"}

library(readr)
CEData <- read_csv("CEsample.csv")
g1 <- ggplot(CEData, aes(x = log_TotalIncome, y = log_TotalExp)) +
  geom_point(size=1) + 
  labs(x = "log(Income)", y = "log(Expenditure)") +
  theme_grey(base_size = 10, base_family = "") 
g1
```


# The CE sample


# A simple linear regression for the CE sample

# MCMC simulation by JAGS for the SLR model

## JAGS script for the SLR model

```{r message = FALSE}
modelString <-"
model {
## sampling
for (i in 1:N){
y[i] ~ dnorm(beta0 + beta1*x[i], invsigma2)
}

## priors
beta0 ~ dnorm(mu0, g0)
beta1 ~ dnorm(mu1, g1)
invsigma2 ~ dgamma(a, b)
sigma <- sqrt(pow(invsigma2, -1))
}
"
```

- Pass the data and hyperparameter values to JAGS:

```{r message = FALSE}
y <- as.vector(CEData$log_TotalExp)
x <- as.vector(CEData$log_TotalIncome)
N <- length(y)
the_data <- list("y" = y, "x" = x, "N" = N,
                 "mu0" = 0, "g0" = 0.0001,
                 "mu1" = 0, "g1" = 0.0001,
                 "a" = 1, "b" = 1)

initsfunction <- function(chain){
  .RNG.seed <- c(1,2)[chain]
  .RNG.name <- c("base::Super-Duper",
                 "base::Wichmann-Hill")[chain]
  return(list(.RNG.seed=.RNG.seed,
              .RNG.name=.RNG.name))
}
```


- Run the JAGS code for this model:

```{r message = FALSE, warning = FALSE}
posterior <- run.jags(modelString,
                      n.chains = 1,
                      data = the_data,
                      monitor = c("beta0", "beta1", "sigma"),
                      adapt = 1000,
                      burnin = 5000,
                      sample = 5000,
                      thin = 1,
                      inits = initsfunction)
```


## JAGS output for the SLR model

- Obtain posterior summaries of all parameters:

```{r message = FALSE, warning = FALSE}
summary(posterior) 
```


```{r fig.align = "center", message = FALSE}
plot(posterior, vars = "beta0")
```


```{r fig.align = "center", message = FALSE}
plot(posterior, vars = "beta1")
```

```{r fig.align = "center", message = FALSE}
plot(posterior, vars = "sigma")
```


## New JAGS script for the SLR model

Setting \texttt{thin = 50}, to get rid of the stickiness in $\beta_0$ and $\beta_1$.

```{r message = FALSE,warning = FALSE}
posterior_new <- run.jags(modelString,
                      n.chains = 1,
                      data = the_data,
                      monitor = c("beta0", "beta1", "sigma"),
                      adapt = 1000,
                      burnin = 5000,
                      sample = 5000,
                      thin = 50,
                      inits = initsfunction)
```


## New JAGS output for the SLR model

- Obtain posterior summaries of all parameters:


```{r message = FALSE, warning = FALSE}
summary(posterior_new) 
```


```{r fig.align = "center", message = FALSE}
plot(posterior_new, vars = "beta0")
```


```{r fig.align = "center", message = FALSE}
plot(posterior_new, vars = "beta1")
```


```{r fig.align = "center", message = FALSE}
plot(posterior_new, vars = "sigma")
```


# Bayesian inferences with SLR

## Simulate fits from the regression model


```{r message = FALSE, warning = FALSE}
post <- as.mcmc(posterior_new)
post_means <- apply(post, 2, mean)
post <- as.data.frame(post)
```

```{r fig.align = "center", message = FALSE}
ggplot(CEData, aes(log_TotalIncome, log_TotalExp)) +
  geom_point(size=1) +
  geom_abline(data=post[1:10, ],
              aes(intercept=beta0, slope=beta1), alpha = 0.5) +
  geom_abline(intercept = post_means[1],
              slope = post_means[2], size = 1) +
  ylab("log(Expenditure)") + xlab("log(Income)") +
  theme_grey(base_size = 10, base_family = "")
```


## Learning about the expected response


```{r message = FALSE, warning = FALSE}
post <- as.data.frame(post)
one_expected <- function(x){
  lp <- post[ , "beta0"] +  x * post[ , "beta1"]
  data.frame(Value = paste("log(Income) =", x),
             Expected_logExp = lp)
}

df <- map_df(c(1, 5, 7, 9), one_expected)
```


```{r fig.align = "center", message = FALSE}
require(ggridges)
ggplot(df, aes(x = Expected_logExp, y = Value)) +
  geom_density_ridges() +
  theme_grey(base_size = 8, base_family = "")
```

```{r message = FALSE, warning = FALSE}
df <- map_df(c(10, 11, 12, 13), one_expected)
ggplot(df, aes(x = Expected_logExp, y = Value)) +
  geom_density_ridges() +
  theme_grey(base_size = 8, base_family = "")
```

```{r message = FALSE, warning = FALSE}
df %>% group_by(Value) %>%
  summarize(P05 = quantile(Expected_logExp, 0.05),
            P50 = median(Expected_logExp),
            P95 = quantile(Expected_logExp, 0.95))
```


## Prediction of future responses

```{r message = FALSE, warning = FALSE}
one_predicted <- function(x){
  lp <- post[ , "beta0"] +  x * post[ , "beta1"]
  y <- rnorm(5000, lp, post[, "sigma"])
  data.frame(Value = paste("Size =", x),
             Predicted_logExp = y)
}
df <- map_df(c(1, 5, 7, 9), one_predicted)
```


```{r fig.align = "center", message = FALSE}
require(ggridges)
ggplot(df, aes(x = Predicted_logExp, y = Value)) +
  geom_density_ridges() +
  theme_grey(base_size = 9, base_family = "")
```



```{r message = FALSE, warning = FALSE}
df %>% group_by(Value) %>%
  summarize(P05 = quantile(Predicted_logExp, 0.05),
            P50 = median(Predicted_logExp),
            P95 = quantile(Predicted_logExp, 0.95))
```


# More on priors

## Subjective prior: standardization

```{r message = FALSE, warning = FALSE}
CEData$log_TotalExpSTD <- scale(CEData$log_TotalExp)
CEData$log_TotalIncomeSTD <- scale(CEData$log_TotalIncome)
```


```{r fig.align = "center", message = FALSE}
g2 = ggplot(CEData, aes(x = log_TotalIncomeSTD, y = log_TotalExpSTD)) +
  geom_point(size=1) + 
  xlab("log(Income) STD") + ylab("log(Expenditure) STD") +
  theme_grey(base_size = 10, base_family = "") 
grid.arrange(g1, g2, ncol=2)
```

## Subjective prior: JAGS script for the standardized SLR model

```{r message = FALSE}
modelString <-"
model {
## sampling
for (i in 1:N){
y[i] ~ dnorm(beta0 + beta1*x[i], invsigma2)
}

## priors
beta0 ~ dnorm(mu0, g0)
beta1 ~ dnorm(mu1, g1)
invsigma2 ~ dgamma(a, b)
sigma <- sqrt(pow(invsigma2, -1))
}
"
```

- Pass the data and hyperparameter values to JAGS:

```{r message = FALSE}
y <- as.vector(CEData$log_TotalExpSTD)
x <- as.vector(CEData$log_TotalIncomeSTD)
N <- length(y)
the_data <- list("y" = y, "x" = x, "N" = N,
                 "mu0" = 0, "g0" = 1,
                 "mu1" = 0.7, "g1" = 44.4,
                 "a" = 1, "b" = 1)

initsfunction <- function(chain){
  .RNG.seed <- c(1,2)[chain]
  .RNG.name <- c("base::Super-Duper",
                 "base::Wichmann-Hill")[chain]
  return(list(.RNG.seed=.RNG.seed,
              .RNG.name=.RNG.name))
}
```



- Run the JAGS code for this model:

```{r message = FALSE, warning = FALSE}
posterior_sub <- run.jags(modelString,
                      n.chains = 1,
                      data = the_data,
                      monitor = c("beta0", "beta1", "sigma"),
                      adapt = 1000,
                      burnin = 5000,
                      sample = 5000,
                      thin = 1,
                      inits = initsfunction)
```

## Subjective prior: JAGS output for the SLR model

- Obtain posterior summaries of all parameters:

\vspace{3mm}

```{r message = FALSE, warning = FALSE}
summary(posterior_sub) 
```


```{r fig.align = "center", message = FALSE}
plot(posterior_sub, vars = "beta1")
```

## Conditional means prior: JAGS script

```{r message = FALSE}
modelString <-"
model {
## sampling
for (i in 1:N){
y[i] ~ dnorm(beta0 + beta1*x[i], invsigma2)
}

## priors
beta1 <- (mu2 - mu1)/(x2 - x1)
beta0 <- mu1 - x1*(mu2 - mu1)/(x2 - x1)
mu1 ~ dnorm(m1, g1)
mu2 ~ dnorm(m2, g2)
invsigma2 ~ dgamma(a, b)
sigma <- sqrt(pow(invsigma2, -1))
}
"
```